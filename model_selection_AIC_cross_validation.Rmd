---
title: 'Model Selection: AIC and Cross Validation'
author: "Sachiko Lamen"
date: "1/13/2022"
output: html_document
---

```{r setup, include=TRUE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(palmerpenguins)
library(AICcmodavg)
library(equatiomatic)
```

# Objectives
* Practice comparing the performance of different linear regression models using AIC and cross-validation
* learn to use formulas in R models
* use a for-loop to perform cross-validaiton manually

# Create a model that will predict penguin mass based on observable characteristics
Create a model we can use in the field to estimate penguin mass based on readily observable characteristics, based on data in the `palmerpenguins` packages

```{r}
penguins_clean <- penguins %>%
  drop_na() %>%
  rename(mass = body_mass_g,
         bill_l = bill_length_mm,
         bill_d = bill_depth_mm,
         flipper_l = flipper_length_mm) # rename column names to make it easier to work with

mdl1 <- lm(mass ~ bill_l + bill_d + flipper_l + species + sex + island,
           data = penguins_clean) # `~` means 'as a function of'

# see that model predicts 87% of variation, lots of parameter estimates have high levels of significance and others do not. This could help decide what variables to drop or keep. Overall model has pretty good p value
```

Lets use a formula!

```{r}
f1 <- mass ~ bill_l + bill_d + flipper_l + species + sex + island # now a stored object to be used later!
mdl1 <- lm(f1, data = penguins_clean)

f2 <- mass ~ bill_l + bill_d + flipper_l + species + sex
mdl2 <- lm(f2, data = penguins_clean)

f3 <- mass ~ bill_d + flipper_l + species +sex
mdl3 <- lm(f3, data = penguins_clean)
```

Which of the three models is the "best"? Best fit with least complexity

# Lets use AIC to compare
- AIC function is built in to R

```{r}
AIC(mdl1, mdl2, mdl3)

# df = 2+number of parameters its trying to estimate
# it looks like model 2 is best (lowest AIC)

# keep in mind that if you had a small sample size (not this case), you would want to correct for having a small sample size using `AICc()` 
AICcmodavg::AICc(mdl1) # AICc: 4727.925 versus AIC: 4727.242

# can directly compare all three using `aictab(list(x,y,z))` that makes a nice viewing table 
AICcmodavg::aictab(list(mdl1, mdl2, mdl3))
# gives list in order of best to worst AIC and gives K value and ∆AIC  (remember you want at least 2 unit difference to consider one better than the other) :: model 2 is considered better than model 1 or 3, model 1 and 3 do not differ significantly.
# AICcWt tells us the weight of the difference, bigger ∆AIC, means bigger weight of the evidence for the model. in this case 80% of the weight of evidence suggest model 2.

```

*AIC is only estimating the parameters for data that you have, might not be good at predicting parameters/best model for other data sets!*

Cross validation is a way to try to predict the performance of the model vased on data it HASNT SEEN YET.

# Lets try K-fold Cross-Validation
K = how many slices you will have in your data

```{r}
folds <- 10 # how many folds you will have in your data
fold_vec <- rep(1:folds, length.out = nrow(penguins_clean)) # rep() is repeat function until it meets criteria, length.out() until it gets to the number of observation (333)
table(fold_vec) # shows a table of values for each fold??

# we will scramble this and use it to assign different observations in the dataset randomly to different folds. 

set.seed(42) # whenever working with random numbers its a good idea to set a seed so if someone else comes along and wants to verify your process, they can get exactly the same random numbers. 

penguins_fold <- penguins_clean %>%
  mutate(group = sample(fold_vec, size = n(), replace = FALSE))
# add new column named 'group' that contains the samples from the fold_vec that has a samaple size of the data frame (333), replace = FALSE guarantees that we will end up with exact distribution as seen in table
         
### First Fold
test_df <- penguins_fold %>%
  filter(group == 1)

training_df <- penguins_fold %>%
  filter(group != 1)
```

Need to have a way of scoring models, create funciton:
*Root Mean Square Error* go in reverse order of these operations. Why is the anagram fucked up... cause why not?
- find error (predicted - actual)
- square it
- find the average
- take the square root

```{r root mean square error function}
calc_rmse <- function(x,y) {rmse_result <- (x - y)^2 %>% mean() %>% sqrt()
return(rmse_result)}
```

Use training dataset to create three linear regression models based on the formulas above

```{r}
training_mdl1 <- lm(f1, data = training_df)
training_mdl2 <- lm(f2, data = training_df)
training_mdl3 <- lm(f3, data = training_df)
```

Use trained models to predict on test data
```{r}
predict_test <- test_df %>%
  mutate(model1 = predict(training_mdl1, test_df),
         model2 = predict(training_mdl2, test_df),
         model3 = predict(training_mdl3, test_df)) # predict() takes parameters from model object and uses it to predict based on a different data frame

rmse_predict_test <- predict_test %>%
  summarize(rmse_mdl1 = calc_rmse(model1, mass),
            rmse_mdl2 = calc_rmse(model2, mass),
            rmse_mdl3 = calc_rmse(model3, mass))

rmse_predict_test
#  rmse_mdl1 rmse_mdl2 rmse_mdl3
#      <dbl>     <dbl>     <dbl>
# 1      326.      319.      327.

# we see that model2 is best for this fold.
```

Lets calculate over ALL folds and take an average (use a for loop)

```{r}
rmse_df <-data.frame() # data.frame() is just a blank data frame

for(i in 1:folds) {
  kfold_test_df <- penguins_fold %>%
    filter(group == i)
  kfold_train_df <- penguins_fold %>%
    filter(group != i)
  
  kfold_mdl1 <- lm(f1, data = kfold_train_df)
  kfold_mdl2 <- lm(f2, data = kfold_train_df)
  kfold_mdl3 <- lm(f3, data = kfold_train_df)
  
  kfold_pred_df <- kfold_test_df %>%
    mutate(mdl1 = predict(kfold_mdl1, kfold_test_df),
           mdl2 = predict(kfold_mdl2, .),
           mdl3 = predict(kfold_mdl3, .))
  kfold_rmse <- kfold_pred_df %>%
    summarize(rmse_mdl1 = calc_rmse(mdl1, mass),
              rmse_mdl2 = calc_rmse(mdl2, mass),
              rmse_mdl3 = calc_rmse(mdl3, mass))
  
  rmse_df <- bind_rows(rmse_df, kfold_rmse) # this will add new row on bottom of data frame each time through the loop
}

rmse_df %>%
  summarise(mean_rmse_mdl1 = mean(rmse_mdl1),
            mean_rmse_mdl2 = mean(rmse_mdl2),
            mean_rmse_mdl3 = mean(rmse_mdl3))
```

# Once we've chosen the model via cross-validation: 
Go back and train the final model (`final_mdl`) based on the formula for the model that you have chosen (`f2`)

```{r}
final_mdl <-lm(f2, data = penguins_clean)

summary(final_mdl)
```
Our Final Model with VARIABLES
`r equatiomatic::extract_eq(final_mdl, wrap = TRUE)` use extract_eq() to nicely format formula easily

And with NUMBERS
`r equatiomatic::extract_eq(final_mdl, wrap = TRUE, use_coefs = TRUE)`




